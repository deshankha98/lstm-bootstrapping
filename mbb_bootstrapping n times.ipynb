{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yt head :\n",
      "0    393.0777\n",
      "1    391.6012\n",
      "2    390.7403\n",
      "3    391.8214\n",
      "4    394.3039\n",
      "Name: close, dtype: float64\n",
      "             yt        yt_       vt        yt1        yt2        yt3\n",
      "0      393.0777   391.6012  6031199   390.4551   393.7283   390.1698\n",
      "1      391.6012   390.7403  4330781   389.5892   391.8915   387.2619\n",
      "2      390.7403   391.8214  3714176   391.2659   394.3440   390.0747\n",
      "3      391.8214   394.3039  2393946   390.4551   393.0677   390.3750\n",
      "4      394.3039   396.8414  3466971   390.2549   394.7644   389.2739\n",
      "5      396.8414   403.8284  5453980   394.0937   397.0266   393.9285\n",
      "6      403.8284   396.6262  5857528   398.3930   403.9035   398.0376\n",
      "7      396.6262   398.1628  5522500   403.0527   404.8895   396.2929\n",
      "8      398.1628   400.2549  7008464   399.3990   403.1277   396.0056\n",
      "9      400.2549   395.7804  4103315   400.0296   401.0256   397.2969\n",
      "10     395.7804   395.4601  4602925   401.5512   404.6092   395.6402\n",
      "11     395.4601   400.2899  4399988   397.8975   398.3729   392.5922\n",
      "12     400.2899   401.0006  4047784   397.7974   402.7774   395.9506\n",
      "13     401.0006   403.4981  4526218   400.9506   403.8985   400.9155\n",
      "14     403.4981   411.1628  4346304   399.2989   403.9736   398.4731\n",
      "15     411.1628   419.7213  5545624   403.0527   411.8314   402.9025\n",
      "16     419.7213   416.1057  8079065   414.8795   420.4950   414.8644\n",
      "17     416.1057   416.7143  5739956   420.9330   422.4220   414.8194\n",
      "18     416.7143   416.1758  4101301   417.4470   418.7283   415.2048\n",
      "19     416.1758   417.8274  5817848   417.6673   417.8774   412.9375\n",
      "20     417.8274   414.2188  3186118   416.2608   420.2699   416.1658\n",
      "21     414.2188   413.0677  4012146   415.7704   416.3609   412.2468\n",
      "22     413.0677   411.1808  3279147   414.3639   415.7603   411.5662\n",
      "23     411.1808   407.5582  3298723   413.9085   413.9085   409.1037\n",
      "24     407.5582   404.2989  6191734   409.6592   410.5602   407.0767\n",
      "25     404.2989   406.0657  3672831   402.9025   406.7864   401.1357\n",
      "26     406.0657   407.7614  4191219   406.0256   410.0346   403.6282\n",
      "27     407.7614   406.0356  2924120   408.8234   409.1638   406.1257\n",
      "28     406.0356   405.5602  2930649   406.0506   408.8671   405.3299\n",
      "29     405.5602   405.2248  2973343   407.7774   408.0276   405.2248\n",
      "...         ...        ...      ...        ...        ...        ...\n",
      "1228  1068.8600  1065.8500   889446  1070.0000  1071.7200  1067.6400\n",
      "1229  1065.8500  1060.2000   918767  1068.6400  1068.8600  1058.6400\n",
      "1230  1060.2000  1055.9500  1116203  1066.6000  1068.2700  1058.3800\n",
      "1231  1055.9500  1053.4000   994249  1062.2500  1064.8400  1053.3800\n",
      "1232  1053.4000  1073.2100  1180340  1055.4900  1058.0500  1052.7000\n",
      "1233  1073.2100  1091.5200  1588268  1053.0200  1075.9800  1053.0200\n",
      "1234  1091.5200  1095.7600  1565945  1073.9300  1096.1000  1073.4300\n",
      "1235  1095.7600  1110.2900  1302569  1097.0900  1104.0800  1094.2600\n",
      "1236  1110.2900  1114.2100  1512526  1103.4500  1113.5800  1101.8000\n",
      "1237  1114.2100  1112.7900  1232221  1111.0000  1119.1600  1110.0000\n",
      "1238  1112.7900  1110.1400  1340381  1118.4400  1118.4400  1108.2000\n",
      "1239  1110.1400  1112.0500  1036655  1107.0000  1112.7800  1103.9800\n",
      "1240  1112.0500  1130.6500  1121216  1112.3100  1114.8500  1106.4800\n",
      "1241  1130.6500  1130.7000  1929306  1110.1000  1131.3000  1108.0100\n",
      "1242  1130.7000  1139.1000  1823100  1140.3100  1148.8800  1126.6600\n",
      "1243  1139.1000  1135.9700  1391510  1136.3600  1139.3200  1123.4900\n",
      "1244  1135.9700  1143.5000  1374873  1139.3500  1140.5900  1124.4600\n",
      "1245  1143.5000  1164.1600  1527554  1138.0300  1143.7800  1132.5000\n",
      "1246  1164.1600  1176.1700  1477520  1143.8200  1166.8800  1141.8200\n",
      "1247  1176.1700  1171.2900  1956865  1170.6200  1178.5100  1167.2500\n",
      "1248  1171.2900  1182.1400  1856429  1184.9800  1187.0500  1167.4000\n",
      "1249  1182.1400  1187.5600  1499247  1180.7100  1185.0000  1171.8400\n",
      "1250  1187.5600  1186.4800  2108502  1187.5300  1187.5600  1168.0300\n",
      "1251  1186.4800  1177.3700  1574708  1188.0000  1198.0000  1184.0600\n",
      "1252  1177.3700  1182.2200  1866883  1177.7200  1187.9300  1174.5100\n",
      "1253  1182.2200  1181.5900  1801135  1183.8100  1186.3200  1172.1000\n",
      "1254  1181.5900  1119.2000  3675709  1175.9900  1187.4500  1169.3600\n",
      "1255  1119.2000  1062.3900  5892122  1127.4200  1131.3000  1111.1700\n",
      "1256  1062.3900  1084.4300  4177469  1100.6100  1114.9900  1056.7400\n",
      "1257  1084.4300  1055.4100  3831524  1033.9800  1087.3800  1030.0100\n",
      "\n",
      "[1258 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shankhajyoti\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:149: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(1000, activation=\"tanh\", input_shape=(1, 1), recurrent_activation=\"hard_sigmoid\")`\n",
      "C:\\Users\\shankhajyoti\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:151: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"linear\", units=1)`\n",
      "C:\\Users\\shankhajyoti\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:154: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "800/800 [==============================] - 12s 15ms/step - loss: 0.1443\n",
      "rmse_train  rmse_test 0.19887051406274392 139.78161158588244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shankhajyoti\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:149: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(1000, activation=\"tanh\", input_shape=(1, 1), recurrent_activation=\"hard_sigmoid\")`\n",
      "C:\\Users\\shankhajyoti\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:151: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"linear\", units=1)`\n",
      "C:\\Users\\shankhajyoti\\Anaconda2\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:154: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "800/800 [==============================] - 12s 15ms/step - loss: 0.1554\n",
      "rmse_train  rmse_test 0.1892985016241401 128.30915103067872\n",
      "rmse_final_train   0.194084507843442\n",
      "rmse_final_test 134.04538130828058\n",
      "comparing factor [5260.88452148]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# moving block bootstrap \n",
    "import numpy as np #bootstrapping 1000 times \n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import  Dropout\n",
    "from keras import backend as tf\n",
    "\n",
    "df=pd.read_csv(\"C:\\\\Users\\\\shankhajyoti\\\\Dropbox\\\\Deepak\\\\GOOGL_data.csv\")\n",
    "data_csv=df\n",
    " \n",
    "#how many data we will use \n",
    "# (should not be more than dataset length )\n",
    "data_to_use= len(data_csv)\n",
    " \n",
    "# number of training data\n",
    "# should be less than data_to_use\n",
    "train_end =len(data_csv)-458\n",
    " \n",
    " \n",
    "total_data=len(data_csv)\n",
    " \n",
    "#most recent data is in the end \n",
    "#so need offset\n",
    "start=total_data - data_to_use\n",
    " \n",
    " \n",
    "#currently doing prediction only for 1 step ahead\n",
    "steps_to_predict =1\n",
    " \n",
    "  \n",
    "yt = data_csv.iloc [start:total_data ,4]    #Close price\n",
    "yt1 = data_csv.iloc [start:total_data ,1]   #Open\n",
    "yt2 = data_csv.iloc [start:total_data ,2]   #High\n",
    "yt3 = data_csv.iloc [start:total_data ,3]   #Low\n",
    "vt = data_csv.iloc [start:total_data ,5]    # volume\n",
    " \n",
    " \n",
    "print (\"yt head :\")\n",
    "print (yt.head())\n",
    " \n",
    "yt_ = yt.shift (-1)\n",
    "     \n",
    "data = pd.concat ([yt, yt_, vt, yt1, yt2, yt3], axis =1)\n",
    "data. columns = ['yt', 'yt_', 'vt', 'yt1', 'yt2', 'yt3']\n",
    "     \n",
    "data = data.dropna()\n",
    "     \n",
    "print (data)\n",
    "     \n",
    "# target variable - closed price\n",
    "# after shifting\n",
    "#y = data ['yt_']\n",
    "#print (y) \n",
    "        \n",
    "#       closed,  volume,   open,  high,   low  \n",
    "\n",
    "cols =['yt']\n",
    "#x = data [cols]\n",
    "rmse_mean_trainset=0.0\n",
    "rmse_mean_testset=0.0\n",
    "n_bootstrap=1000\n",
    "testset_array=np.zeros(shape=(n_bootstrap,458))\n",
    "testset_array_sorted=np.zeros(shape=(n_bootstrap,458))\n",
    "\n",
    "ci_lowerbound=np.zeros(shape=(1,458))\n",
    "ci_upperbound=np.zeros(shape=(1,458))\n",
    "n=800\n",
    "\n",
    "mbb_listx=pd.DataFrame() \n",
    "\n",
    "for idx in range(n_bootstrap):\n",
    " l1=2 #optimal block length \n",
    " b1=(n//l1) #no of blocks to be used\n",
    " if (n!=b1*l1):\n",
    "    b1=b1+1\n",
    " N=n-l1+1   \n",
    " ind=np.zeros((N,1),dtype=np.int)\n",
    " for i in range(0,N):\n",
    "    ind[i,0]=i\n",
    " ind=np.array(ind).reshape(len(ind),1)  \n",
    " for p in range(b1):\n",
    "        array=ind.reshape(len(ind))\n",
    "        h=np.random.choice(array,replace=True)\n",
    "        for q in range(h,h+l1):\n",
    "            mbb_listx=mbb_listx.append(data.iloc[q,:],ignore_index=True)\n",
    "           \n",
    " x_train=mbb_listx[cols]   \n",
    " x_test=data[cols].loc[800:1257,:]\n",
    " y_train=mbb_listx['yt_']\n",
    " y_test=data['yt_'].loc[800:1257]\n",
    " mbb_listx=mbb_listx.iloc[0:0,0:0]\n",
    " scaler_x_train = preprocessing.MinMaxScaler ( feature_range =( -1, 1))\n",
    " x_train = np. array (x_train).reshape ((len( x_train) ,len(cols)))\n",
    " x_train_copy=x_train\n",
    "\n",
    "\n",
    " smoothing_window_size = 320\n",
    " for di in range(0,800,smoothing_window_size):\n",
    "     scaler_x_train.fit(x_train[di:di+smoothing_window_size,:])\n",
    "     x_train[di:di+smoothing_window_size,:] = scaler_x_train.transform(x_train[di:di+smoothing_window_size,:])\n",
    " \n",
    "\n",
    " x_test = np. array (x_test).reshape ((len( x_test) ,len(cols)))\n",
    " x_test=scaler_x_train.transform(x_test)\n",
    "\n",
    " x_train = x_train.reshape (x_train. shape + (1,)) \n",
    " x_test = x_test.reshape (x_test. shape + (1,))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " scaler_y_train = preprocessing. MinMaxScaler ( feature_range =( -1, 1))\n",
    " y_train = np.array (y_train).reshape ((len( y_train),1 ))\n",
    " y_train_copy=y_train\n",
    "\n",
    " smoothing_window_size = 320\n",
    " for di in range(0,800,smoothing_window_size):\n",
    "     scaler_y_train.fit(y_train[di:di+smoothing_window_size,:])\n",
    "     y_train[di:di+smoothing_window_size,:] = scaler_y_train.transform(y_train[di:di+smoothing_window_size,:])\n",
    "\n",
    "\n",
    " y_test = np. array (y_test).reshape ((len( y_test) ,1))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    " \n",
    "    \n",
    "\n",
    " \n",
    "     \n",
    "     \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " fit1 = Sequential ()\n",
    " fit1.add (LSTM (  1000 , activation = 'tanh', inner_activation = 'hard_sigmoid' , input_shape =(len(cols), 1) ))\n",
    " fit1.add(Dropout(0.2))\n",
    " fit1.add (Dense (output_dim =1, activation = 'linear'))\n",
    " \n",
    " fit1.compile (loss =\"mean_squared_error\" , optimizer = \"adam\")   \n",
    " fit1.fit (x_train, y_train, batch_size =20, nb_epoch =20, shuffle = False)\n",
    " \n",
    "##print (fit1.summary())\n",
    " x_train_copy = np.array(x_train_copy).reshape (x_train_copy. shape + (1,)) \n",
    "\n",
    " pred1 = fit1.predict (x_train_copy) \n",
    " pred1=np.array(pred1).reshape((len(pred1),1))\n",
    " sum=0.0\n",
    " for i in range(len(y_train)):\n",
    "     sum=sum+(y_train_copy[i,0]-pred1[i,0])**2\n",
    "\n",
    "    \n",
    " sum1=(sum/len(y_train))**0.5\n",
    " rmse_mean_trainset=rmse_mean_trainset+sum1\n",
    "\n",
    " pred1 = fit1.predict (x_test)\n",
    " pred1 = scaler_y_train.inverse_transform (np. array (pred1). reshape ((len( pred1), 1)))    \n",
    "\n",
    " sum=0.0\n",
    " for i in range(len(y_test)):\n",
    "     sum=sum+(y_test[i,0]-pred1[i,0])**2\n",
    "\n",
    "    \n",
    " sum=(sum/len(y_test))**0.5\n",
    " pred1=np.array(pred1).reshape(1,len(pred1))\n",
    " testset_array[idx,:]=pred1\n",
    " rmse_mean_testset=rmse_mean_testset+sum\n",
    " print(\"rmse_train  rmse_test\",sum1,sum)\n",
    " tf.clear_session()\n",
    " \n",
    "  \n",
    "rmse_mean_trainset=rmse_mean_trainset/n_bootstrap\n",
    "rmse_mean_testset=rmse_mean_testset/n_bootstrap\n",
    "print(\"rmse_final_train  \",rmse_mean_trainset)\n",
    "print(\"rmse_final_test\",rmse_mean_testset) \n",
    "testset_array_sorted=np.sort(testset_array,axis=0,kind='quicksort')  \n",
    "avg_pred=np.sum(testset_array_sorted,axis=0)     \n",
    "avg_pred=avg_pred/n_bootstrap \n",
    "avg_pred=np.array(avg_pred).reshape(458,1)\n",
    "ci_lowerbound[0,:]=testset_array_sorted[24,:] \n",
    "ci_upperbound[0,:]=testset_array_sorted[974,:]\n",
    "ci_upperbound=np.array(ci_upperbound).reshape(458,1)\n",
    "ci_lowerbound=np.array(ci_lowerbound).reshape(458,1)\n",
    "comparing_factor=np.sum(ci_upperbound,axis=0)-np.sum(ci_lowerbound,axis=0)\n",
    "\n",
    "print(\"comparing factor\",comparing_factor)\n",
    "\n",
    " \n",
    "\n",
    "import matplotlib.ticker as mtick\n",
    "fmt = '$%.0f'\n",
    "tick = mtick.FormatStrFormatter(fmt)\n",
    " \n",
    "ax = plt.axes()\n",
    "ax.yaxis.set_major_formatter(tick)\n",
    "  \n",
    " \n",
    "plt.plot(avg_pred, label=\"avg_predictions\")\n",
    " \n",
    " \n",
    "\n",
    "plt.plot( [row[0] for row in y_test], label=\"actual\")\n",
    "plt.plot(ci_lowerbound,label=\"lowerbound_ci\")\n",
    "plt.plot(ci_upperbound,label=\"upperbound_ci\")\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "          fancybox=True, shadow=True, ncol=2)\n",
    " \n",
    "\n",
    " \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #bootstrapping 1000 times to compare by checking difference between upper and lower bounds of confidence intervals#\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import  Dropout\n",
    "from keras import backend as tf\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "df=pd.read_csv(\"C:\\\\Users\\\\shankhajyoti\\\\Dropbox\\\\Deepak\\\\GOOGL_data.csv\")\n",
    "data_csv=df\n",
    " \n",
    "#how many data we will use \n",
    "# (should not be more than dataset length )\n",
    "data_to_use= len(data_csv)\n",
    " \n",
    "# number of training data\n",
    "# should be less than data_to_use\n",
    "train_end =len(data_csv)-458\n",
    " \n",
    " \n",
    "total_data=len(data_csv)\n",
    " \n",
    "#most recent data is in the end \n",
    "#so need offset\n",
    "start=total_data - data_to_use\n",
    " \n",
    " \n",
    "#currently doing prediction only for 1 step ahead\n",
    "steps_to_predict =1\n",
    " \n",
    "  \n",
    "yt = data_csv.iloc [start:total_data ,4]    #Close price\n",
    "yt1 = data_csv.iloc [start:total_data ,1]   #Open\n",
    "yt2 = data_csv.iloc [start:total_data ,2]   #High\n",
    "yt3 = data_csv.iloc [start:total_data ,3]   #Low\n",
    "vt = data_csv.iloc [start:total_data ,5]    # volume\n",
    " \n",
    " \n",
    "print (\"yt head :\")\n",
    "print (yt.head())\n",
    " \n",
    "yt_ = yt.shift (-1)\n",
    "     \n",
    "data = pd.concat ([yt, yt_, vt, yt1, yt2, yt3], axis =1)\n",
    "data. columns = ['yt', 'yt_', 'vt', 'yt1', 'yt2', 'yt3']\n",
    "     \n",
    "data = data.dropna()\n",
    "     \n",
    "print (data)\n",
    "     \n",
    "# target variable - closed price\n",
    "# after shifting\n",
    "#y = data ['yt_']\n",
    "#print (y) \n",
    "        \n",
    "#       closed,  volume,   open,  high,   low  \n",
    "\n",
    "cols =['yt']\n",
    "#x = data [cols]\n",
    "rmse_mean_trainset=0.0\n",
    "rmse_mean_testset=0.0\n",
    "n_bootstrap=1000\n",
    "testset_array=np.zeros(shape=(n_bootstrap,458))\n",
    "testset_array_sorted=np.zeros(shape=(n_bootstrap,458))\n",
    "\n",
    "ci_lowerbound=np.zeros(shape=(1,458))\n",
    "ci_upperbound=np.zeros(shape=(1,458))\n",
    "n=800\n",
    "b=1\n",
    "B=0.01\n",
    "mbb_listx=pd.DataFrame() \n",
    "#lbb_listy=pd.DataFrame()\n",
    "for idx in range(n_bootstrap):\n",
    "   \n",
    " l1=2 #optimal block length \n",
    " b1=(n//l1) #no of blocks to be used\n",
    " if (n!=b1*l1):\n",
    "    b1=b1+1\n",
    " N=n-l1+1   \n",
    " ind=np.zeros((N,1),dtype=np.int)\n",
    " for i in range(0,N):\n",
    "    ind[i,0]=i\n",
    " ind=np.array(ind).reshape(len(ind),1)  \n",
    " for p in range(b1):\n",
    "        array=ind.reshape(len(ind))\n",
    "        h=np.random.choice(array,replace=True)\n",
    "        for q in range(h,h+l1):\n",
    "            mbb_listx=mbb_listx.append(data.iloc[q,:],ignore_index=True)\n",
    " \n",
    "#print(lbb_listx) \n",
    " x_train=mbb_listx[cols]   \n",
    " x_test=data[cols].loc[800:1257,:]\n",
    " y_train=mbb_listx['yt_']\n",
    " y_test=data['yt_'].loc[800:1257]\n",
    " mbb_listx=mbb_listx.iloc[0:0,0:0]\n",
    " timesteps=5\n",
    " n_batch=15\n",
    " scaler_x_train = preprocessing.MinMaxScaler ( feature_range =( -1, 1))\n",
    " x_train = np. array (x_train).reshape ((len( x_train) ,len(cols)))\n",
    " x_train_copy=x_train\n",
    " smoothing_window_size = 320\n",
    " for di in range(0,800,smoothing_window_size):\n",
    "    scaler_x_train.fit(x_train[di:di+smoothing_window_size,:])\n",
    "    x_train[di:di+smoothing_window_size,:] = scaler_x_train.transform(x_train[di:di+smoothing_window_size,:])\n",
    "\n",
    "    \n",
    " x_test = np. array (x_test).reshape ((len( x_test) ,len(cols)))\n",
    " x_test_copy=x_test\n",
    " x_test=scaler_x_train.transform(x_test)\n",
    "\n",
    " x_train = x_train.reshape (len(x_train),1) \n",
    " x_test = x_test.reshape (len(x_test),1)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    " scaler_y_train = preprocessing. MinMaxScaler ( feature_range =( -1, 1))\n",
    " y_train = np.array (y_train).reshape ((len( y_train), 1))\n",
    " y_train_copy=y_train\n",
    " smoothing_window_size = 320\n",
    " for di in range(0,800,smoothing_window_size):\n",
    "    scaler_y_train.fit(y_train[di:di+smoothing_window_size,:])\n",
    "    y_train[di:di+smoothing_window_size,:] = scaler_y_train.transform(y_train[di:di+smoothing_window_size,:])\n",
    "    \n",
    "\n",
    "\n",
    " y_test = np. array (y_test).reshape ((len( y_test) ,1))\n",
    " y_test=scaler_y_train.transform(y_test)\n",
    " A=np.append(y_train,y_test,axis=0)\n",
    " A=np.array(A).reshape(len(y_train)+len(y_test),1)\n",
    " lag=timesteps\n",
    " df = DataFrame(A)\n",
    "\n",
    " columns = [df.shift(i) for i in range(lag,0,-1)]\n",
    " columns.append(df)\n",
    " df = concat(columns, axis=1)\n",
    " df=np.array(df).reshape(len(df),timesteps+1)\n",
    " const=len(x_train)\n",
    " const2=len(x_test)\n",
    " a=n_batch-(const2-((const2//n_batch)*n_batch))\n",
    "\n",
    " x_train=df[timesteps:len(x_train),0:-1]\n",
    " y_train=df[timesteps:const,-1]\n",
    " x_test=df[const-a:const+const2,0:-1]\n",
    " y_test=df[const-a:const+const2,-1]\n",
    "\n",
    "\n",
    " \n",
    "     \n",
    "     \n",
    " \n",
    "\n",
    " \n",
    " \n",
    " fit1 = Sequential ()\n",
    " fit1.add (LSTM (  1000 , activation = 'tanh', inner_activation = 'hard_sigmoid' , batch_input_shape =(n_batch,timesteps, 1) ))\n",
    " fit1.add(Dropout(0.2))\n",
    "\n",
    "\n",
    " fit1.add (Dense (output_dim =1, activation = 'linear'))\n",
    " \n",
    " fit1.compile (loss =\"mean_squared_error\" , optimizer = \"adam\")   \n",
    " x_train=np.array(x_train).reshape(len(x_train),timesteps,1)\n",
    "\n",
    " fit1.fit (x_train, y_train, batch_size =n_batch, nb_epoch =20, shuffle = False)\n",
    " \n",
    " print (fit1.summary())\n",
    " y_test=np.array(y_test).reshape(const2+a,1)\n",
    " A=np.append(y_train_copy,y_test,axis=0)\n",
    " A=np.array(A).reshape(len(y_train_copy)+len(y_test),1)\n",
    " lag=timesteps\n",
    " df = DataFrame(A)    \n",
    "    \n",
    " columns = [df.shift(i) for i in range(lag,0,-1)]\n",
    " columns.append(df)\n",
    " df = concat(columns, axis=1)\n",
    " df=np.array(df).reshape(len(df),timesteps+1)\n",
    " x_train_copy=df[timesteps:const,0:-1]\n",
    " y_train_copy=df[timesteps:const,-1]\n",
    "\n",
    " x_train_copy=np.array(x_train_copy).reshape(len(x_train_copy),timesteps,1)\n",
    "\n",
    " pred1= fit1.predict(x_train_copy,batch_size=n_batch)\n",
    " pred1=np.array(pred1).reshape(len(pred1),1)\n",
    " y_train_copy=np.array(y_train_copy).reshape(len(pred1),1)\n",
    "\n",
    "\n",
    " sum=0.0\n",
    " for i in range(len(y_train_copy)):\n",
    "    sum=sum+(y_train_copy[i,0]-pred1[i,0])**2\n",
    "\n",
    "    \n",
    " sum1=(sum/len(y_train))**0.5\n",
    " rmse_mean_trainset=rmse_mean_trainset+sum1\n",
    " \n",
    " x_test=np.array(x_test).reshape(len(x_test),timesteps,1)\n",
    " pred1=fit1.predict(x_test,batch_size=n_batch) \n",
    " \n",
    " pred1 = scaler_y_train.inverse_transform (np. array (pred1). reshape ((len( pred1), 1)))\n",
    "\n",
    " y_test = scaler_y_train.inverse_transform (np. array (y_test). reshape ((len( pred1), 1)))\n",
    " sum=0.0\n",
    " for i in range(a,len(y_test),1):\n",
    "    sum=sum+(y_test[i,0]-pred1[i,0])**2\n",
    "\n",
    "    \n",
    " sum=(sum/len(y_test)-a)**0.5\n",
    " \n",
    " for q in range(a,len(pred1),1):\n",
    "    testset_array[idx,q-a]=pred1[q,0]\n",
    " rmse_mean_testset=rmse_mean_testset+sum\n",
    " print(\"rmse_train  rmse_test\",sum1,sum)\n",
    " tf.clear_session()\n",
    " \n",
    "\n",
    "\n",
    "      \n",
    "    \n",
    " \n",
    " \n",
    " \n",
    "  \n",
    "rmse_mean_trainset=rmse_mean_trainset/n_bootstrap\n",
    "rmse_mean_testset=rmse_mean_testset/n_bootstrap\n",
    "print(\"rmse_final_train  \",rmse_mean_trainset)\n",
    "print(\"rmse_final_test\",rmse_mean_testset) \n",
    "testset_array_sorted=np.sort(testset_array,axis=0,kind='quicksort')  \n",
    "avg_pred=np.sum(testset_array_sorted,axis=0)     \n",
    "avg_pred=avg_pred/n_bootstrap \n",
    "avg_pred=np.array(avg_pred).reshape(458,1)\n",
    "ci_lowerbound[0,:]=testset_array_sorted[24,:] \n",
    "ci_upperbound[0,:]=testset_array_sorted[974,:]\n",
    "ci_upperbound=np.array(ci_upperbound).reshape(458,1)\n",
    "ci_lowerbound=np.array(ci_lowerbound).reshape(458,1)\n",
    "comparing_factor=np.sum(ci_upperbound,axis=0)-np.sum(ci_lowerbound,axis=0)\n",
    "\n",
    "print(\"comparing factor\",comparing_factor)\n",
    "\n",
    " \n",
    "\n",
    "import matplotlib.ticker as mtick\n",
    "fmt = '$%.0f'\n",
    "tick = mtick.FormatStrFormatter(fmt)\n",
    " \n",
    "ax = plt.axes()\n",
    "ax.yaxis.set_major_formatter(tick)\n",
    "  \n",
    " \n",
    "plt.plot(avg_pred, label=\"avg_predictions\")\n",
    " \n",
    " \n",
    "\n",
    "plt.plot( [row[0] for row in y_test], label=\"actual\")\n",
    "plt.plot(ci_lowerbound,label=\"lowerbound_ci\")\n",
    "plt.plot(ci_upperbound,label=\"upperbound_ci\")\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "          fancybox=True, shadow=True, ncol=2)\n",
    " \n",
    "\n",
    " \n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
